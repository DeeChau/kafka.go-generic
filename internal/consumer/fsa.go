// Code generated by github.com/wishabi/kafka.go. DO NOT EDIT.
package consumers

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"time"

	"github.com/segmentio/kafka-go"
	"github.com/wishabi/kafka.go/avro"

	schema "github.com/DeeChau/kafka.go-generic/internal/schema"
)

// FsaMessage contains Avro Key and Value encoded version of Schema Fsa
type FsaMessage struct {
	Topic     string
	Partition int
	Offset    int64
	Key       *schema.FsaKey
	Value     *schema.Fsa
	Headers   []kafka.Header
	Time      time.Time
}

func parseFsaMessage(message kafka.Message, registry *avro.Registry) (*FsaMessage, error) {
	valueData, valueSchemaID, err := avro.Parse(message.Value)
	if err != nil && !errors.Is(err, avro.ErrEmpty) {
		return nil, fmt.Errorf("can't parse Fsa: %w", err)
	}

	var value *schema.Fsa

	// if value is tombstone message, the error will be avro.ErrEmpty
	// if it's not tombdstone message, then we need to parse the value
	if !errors.Is(err, avro.ErrEmpty) {
		valueSchema, err := registry.Schema(valueSchemaID)
		value, err = schema.DeserializeFsaFromSchema(bytes.NewReader(valueData), valueSchema)
		if err != nil {
			return nil, fmt.Errorf("failed to deserialize Fsa message: %w", err)
		}
	}

	keyData, keySchemaID, err := avro.Parse(message.Key)
	if err != nil && !errors.Is(err, avro.ErrEmpty) {
		return nil, fmt.Errorf("can't parse FsaKey: %w", err)
	}

	var key *schema.FsaKey

	// if key is tombstone message, the error will be avro.ErrEmpty
	// if it's not tombdstone message, then we need to parse the key
	// NOTE: if the key is empty, there must be an error happened in publisher side
	// whoever receiving this message should consider adding null check at key
	if !errors.Is(err, avro.ErrEmpty) {
		keySchema, err := registry.Schema(keySchemaID)

		key, err = schema.DeserializeFsaKeyFromSchema(bytes.NewReader(keyData), keySchema)
		if err != nil {
			return nil, fmt.Errorf("failed to deserialize FsaKey message: %w", err)
		}		
	}

	return &FsaMessage{
		Topic:     message.Topic,
		Partition: message.Partition,
		Offset:    message.Offset,
		Key:       key,
		Value:     value,
		Headers:   message.Headers,
		Time:      message.Time,
	}, nil
}

// AvroFsaConsumer kafka consumer for the Schema Type Fsa
type AvroFsaConsumer struct {
	reader   *kafka.Reader
	registry *avro.Registry
	topic    string
}

// Consume consumes the next message and returns FsaMessage, commit function and an error
// The caller of this call is responsible to call the commit.
// Note: this method is a blocking call and can be cancled by passing timeout or deadline context
func (c *AvroFsaConsumer) Consume(ctx context.Context) (*FsaMessage, func(context.Context) error, error) {
	message, err := c.reader.FetchMessage(ctx)
	if err != nil {
		return nil, nil, fmt.Errorf("failed to read Fsa message: %w", err)
	}

	commit := func(ctx context.Context) error {
		return c.reader.CommitMessages(ctx, message)
	}

	msg, err := parseFsaMessage(message, c.registry)
	if err != nil {
		return nil, commit, err
	}

	return msg, commit, nil
}

// Consumes returns batch of messages. There are 3 arguments that might effect the number of return results
// 1: context with timeout, deadline or canceled
// 2: the maximum number of messages
// 3: the configured min and max size of consumer which pass by kafka.ReaderConfig
func (c *AvroFsaConsumer) Consumes(ctx context.Context, max int) ([]*FsaMessage, func(ctx context.Context) error, error) {
	raw := make([]kafka.Message, 0)
	result := make([]*FsaMessage, 0)

	commit := func(ctx context.Context) error {
		return c.reader.CommitMessages(ctx, raw...)
	}

	for i := 0; i < max; i++ {
		select {
		case <-ctx.Done():
			break
		default:
			msg, err := c.reader.FetchMessage(ctx)
			if err != nil {
				return nil, commit, err
			}
			raw = append(raw, msg)
			message, err := parseFsaMessage(msg, c.registry)
			if err != nil {
				return nil, commit, err
			}
			result = append(result, message)
		}
	}

	return result, commit, nil
}

// AutoCommitConsume consumes the next message and returns FsaMessage and automatically calls commit
// by passing a timeout context, ones can control timeout of consuming and commiting a message
func (c *AvroFsaConsumer) AutoCommitConsume(ctx context.Context) (*FsaMessage, error) {
	msg, commit, err := c.Consume(ctx)
	if err != nil {
		return nil, err
	}

	return msg, commit(ctx)
}

// NewAvroFsaConsumer creates a consumer which can consume and returns FsaMessage messages
func NewAvroFsaConsumer(config kafka.ReaderConfig, registry *avro.Registry) *AvroFsaConsumer {
	return &AvroFsaConsumer{
		reader:   kafka.NewReader(config),
		registry: registry,
		topic:    config.Topic,
	}
}

