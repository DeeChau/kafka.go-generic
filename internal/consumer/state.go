// Code generated by github.com/wishabi/kafka.go. DO NOT EDIT.
package consumers

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"time"

	"github.com/segmentio/kafka-go"
	"github.com/wishabi/kafka.go/avro"

	schema "github.com/DeeChau/kafka.go-generic/internal/schema"
)

// StateMessage contains Avro Key and Value encoded version of Schema State
type StateMessage struct {
	Topic     string
	Partition int
	Offset    int64
	Key       *schema.StateKey
	Value     *schema.State
	Headers   []kafka.Header
	Time      time.Time
}

func parseStateMessage(message kafka.Message, registry *avro.Registry) (*StateMessage, error) {
	valueData, valueSchemaID, err := avro.Parse(message.Value)
	if err != nil && !errors.Is(err, avro.ErrEmpty) {
		return nil, fmt.Errorf("can't parse State: %w", err)
	}

	var value *schema.State

	// if value is tombstone message, the error will be avro.ErrEmpty
	// if it's not tombdstone message, then we need to parse the value
	if !errors.Is(err, avro.ErrEmpty) {
		valueSchema, err := registry.Schema(valueSchemaID)
		value, err = schema.DeserializeStateFromSchema(bytes.NewReader(valueData), valueSchema)
		if err != nil {
			return nil, fmt.Errorf("failed to deserialize State message: %w", err)
		}
	}

	keyData, keySchemaID, err := avro.Parse(message.Key)
	if err != nil && !errors.Is(err, avro.ErrEmpty) {
		return nil, fmt.Errorf("can't parse StateKey: %w", err)
	}

	var key *schema.StateKey

	// if key is tombstone message, the error will be avro.ErrEmpty
	// if it's not tombdstone message, then we need to parse the key
	// NOTE: if the key is empty, there must be an error happened in publisher side
	// whoever receiving this message should consider adding null check at key
	if !errors.Is(err, avro.ErrEmpty) {
		keySchema, err := registry.Schema(keySchemaID)

		key, err = schema.DeserializeStateKeyFromSchema(bytes.NewReader(keyData), keySchema)
		if err != nil {
			return nil, fmt.Errorf("failed to deserialize StateKey message: %w", err)
		}		
	}

	return &StateMessage{
		Topic:     message.Topic,
		Partition: message.Partition,
		Offset:    message.Offset,
		Key:       key,
		Value:     value,
		Headers:   message.Headers,
		Time:      message.Time,
	}, nil
}

// AvroStateConsumer kafka consumer for the Schema Type State
type AvroStateConsumer struct {
	reader   *kafka.Reader
	registry *avro.Registry
	topic    string
}

// Consume consumes the next message and returns StateMessage, commit function and an error
// The caller of this call is responsible to call the commit.
// Note: this method is a blocking call and can be cancled by passing timeout or deadline context
func (c *AvroStateConsumer) Consume(ctx context.Context) (*StateMessage, func(context.Context) error, error) {
	message, err := c.reader.FetchMessage(ctx)
	if err != nil {
		return nil, nil, fmt.Errorf("failed to read State message: %w", err)
	}

	commit := func(ctx context.Context) error {
		return c.reader.CommitMessages(ctx, message)
	}

	msg, err := parseStateMessage(message, c.registry)
	if err != nil {
		return nil, commit, err
	}

	return msg, commit, nil
}

// Consumes returns batch of messages. There are 3 arguments that might effect the number of return results
// 1: context with timeout, deadline or canceled
// 2: the maximum number of messages
// 3: the configured min and max size of consumer which pass by kafka.ReaderConfig
func (c *AvroStateConsumer) Consumes(ctx context.Context, max int) ([]*StateMessage, func(ctx context.Context) error, error) {
	raw := make([]kafka.Message, 0)
	result := make([]*StateMessage, 0)

	commit := func(ctx context.Context) error {
		return c.reader.CommitMessages(ctx, raw...)
	}

	for i := 0; i < max; i++ {
		select {
		case <-ctx.Done():
			break
		default:
			msg, err := c.reader.FetchMessage(ctx)
			if err != nil {
				return nil, commit, err
			}
			raw = append(raw, msg)
			message, err := parseStateMessage(msg, c.registry)
			if err != nil {
				return nil, commit, err
			}
			result = append(result, message)
		}
	}

	return result, commit, nil
}

// AutoCommitConsume consumes the next message and returns StateMessage and automatically calls commit
// by passing a timeout context, ones can control timeout of consuming and commiting a message
func (c *AvroStateConsumer) AutoCommitConsume(ctx context.Context) (*StateMessage, error) {
	msg, commit, err := c.Consume(ctx)
	if err != nil {
		return nil, err
	}

	return msg, commit(ctx)
}

// NewAvroStateConsumer creates a consumer which can consume and returns StateMessage messages
func NewAvroStateConsumer(config kafka.ReaderConfig, registry *avro.Registry) *AvroStateConsumer {
	return &AvroStateConsumer{
		reader:   kafka.NewReader(config),
		registry: registry,
		topic:    config.Topic,
	}
}

